{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d80d1d06-5d69-49cb-9ae5-b0902438242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efcc22-e975-4c5a-a899-16c67daeb52b",
   "metadata": {},
   "source": [
    "# Подгружаем все размеченные аспекты из файлов и сохраняем в памяти\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "589d5229-dc7a-4ad6-a80a-36e40cf1cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['лекции', 'задания, задачи', 'практики, семинары', 'тесты', 'домашняя работа', 'материал, информация, темы', 'преподаватель'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIRECTORY = Path(\"../data/otzyvus-annotated/\")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "# Предварительно удаляем расщирение .txt\n",
    "aspect_file_names = [\n",
    "    f[:-4]\n",
    "        for f\n",
    "        in listdir(DATA_DIRECTORY)\n",
    "        if isfile(DATA_DIRECTORY / f) and \".txt\" in f\n",
    "]\n",
    "\n",
    "aspect_file_names = filter(\n",
    "    lambda name: name in [\n",
    "        \"преподаватель\",\n",
    "        \"материал__информация__темы\",\n",
    "        \"практики__семинары\",\n",
    "        \"лекции\",\n",
    "        \"тесты\",\n",
    "        \"задания__задачи\",\n",
    "        \"домашняя работа\"\n",
    "    ],\n",
    "    aspect_file_names\n",
    ")\n",
    "\n",
    "# Объект аспект - массив предложений\n",
    "actual_aspects_sentences = {}\n",
    "# Список всех уникальных предложений\n",
    "all_aspects_senteces = []\n",
    "for file_name in aspect_file_names:\n",
    "    with open(DATA_DIRECTORY / (file_name + \".txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        key = re.sub(r'[_]+', r', ', file_name).lower()\n",
    "        actual_aspects_sentences[key] = set(\n",
    "            filter(\n",
    "                lambda string: len(string) > 0 and string[0].isalpha(),\n",
    "                f.read().split('\\n')\n",
    "            )\n",
    "        )\n",
    "        all_aspects_senteces += actual_aspects_sentences[key]\n",
    "\n",
    "all_aspects_senteces = list(set(all_aspects_senteces))\n",
    "# actual_aspects_sentences.pop(\"мусор\")\n",
    "# print(actual_aspects_sentences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c95202cb-0b55-4ac0-8a98-9bc51d03bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aspects():\n",
    "    return list(actual_aspects_sentences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ce967d6e-512d-4e4b-a9d4-9e35142fdab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    # Убираем ссылки\n",
    "    clean = re.sub(u'(http|ftp|https):\\/\\/[^ ]+', '', text)\n",
    "    # Убираем все неалфавитные символы кроме дефиса и апострофа\n",
    "    clean = re.sub(u'[^а-я^А-Я^\\w^\\-^\\']', ' ', clean)\n",
    "    # Убираем тире\n",
    "    clean = re.sub(u' - ', ' ', clean)\n",
    "    # Убираем дубликаты пробелов\n",
    "    clean = re.sub(u'\\s+', ' ', clean)\n",
    "    # Убираем пробелы в начале и в конце строки\n",
    "    clean = clean.strip().lower()\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a4d5b5da-4294-437f-ac05-e783e10d7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodSubstring():\n",
    "    dictionary = {\n",
    "        \"материал, информация, темы\": [\"материал\"],\n",
    "        \"домашняя работа\": [\"домашняя работа\"],\n",
    "        \"зачет, экзамен\": [\"зачет\"],\n",
    "        \"фильмы\": [\"фильм\", \"кино\"],\n",
    "        \"презентации\": [\"презентация\"],\n",
    "        \"онлайн-курс\": [\"онлайн-курс\"],\n",
    "        \"видео-уроки\": [\"видео-урок\", \"видеоурок\"],\n",
    "        \"преподаватель\": [\"преподаватель\", \"препод\"],\n",
    "        \"выступления\": [\"выступление\"],\n",
    "        \"литература\": [\"литература\", \"книга\"],\n",
    "        \"тесты\": [\"тест\"],\n",
    "        \"практики, семинары\": [\"практика\"],\n",
    "        \"доклады\": [\"доклад\"],\n",
    "        \"задания, задачи\": [\"задание\"],\n",
    "        \"баллы, оценки\": [\"балл\", \"оценка\"],\n",
    "        \"эссе\": [\"эссе\"],\n",
    "        \"проекты\": [\"проект\"],\n",
    "        \"игры, интерактивность\" : [\"игра\"],\n",
    "        \"лекции\": [\"лекция\"],\n",
    "    }\n",
    "\n",
    "    def set_aspects(self, aspects_list=None):\n",
    "        if aspects_list is not None:\n",
    "            self.aspects_list = aspects_list\n",
    "        else:\n",
    "            self.aspects_list = load_aspects()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.set_aspects(None)\n",
    "\n",
    "    def find_aspects(self, sentence: str):\n",
    "        aspects = []\n",
    "        # Очистка\n",
    "        sentence_words = clean_text(sentence).split(\" \")\n",
    "        # Лемматизация\n",
    "        doc = Doc(sentence)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "\n",
    "        for token in doc.tokens: \n",
    "            token.lemmatize(morph_vocab)\n",
    "        \n",
    "        lemmatized = ' '.join(token.lemma for token in doc.tokens)\n",
    "        \n",
    "        for aspect in self.aspects_list:\n",
    "            for word in self.dictionary[aspect]:\n",
    "                if word in lemmatized:\n",
    "                    aspects.append(aspect)\n",
    "        return aspects\n",
    "\n",
    "    def process(self, text: str):\n",
    "        return self.find_aspects(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "445b263d-cd19-4067-96a0-8d736eff99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodSimilarity():\n",
    "\n",
    "    def transformers_tokenizer(self, sentence: str) -> torch.Tensor:\n",
    "        \n",
    "        tokens = self.transformers_auto_tokenizer(sentence, return_tensors='pt')\n",
    "        vector = self.transformers_model(**tokens)[0].detach().squeeze()\n",
    "        return torch.mean(vector, dim=0).numpy()\n",
    "        # return self.model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    def set_aspects(self, aspects_list=None):\n",
    "        if aspects_list is not None:\n",
    "            self.aspects_list = aspects_list\n",
    "        else:\n",
    "            self.aspects_list = load_aspects()\n",
    "\n",
    "    def __init__(self, tokenizer=\"distiluse\", min_similarity = 0.3):\n",
    "        self.set_aspects(None)\n",
    "        self.aspects_list = load_aspects()\n",
    "        self.min_similarity = min_similarity\n",
    "        if tokenizer == \"distiluse\":\n",
    "            self.tokenizer = self.transformers_tokenizer\n",
    "            \n",
    "            self.transformers_auto_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "            self.transformers_model = AutoModel.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "            # self.model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "        elif tokenizer == \"sbert-pq\":\n",
    "            self.tokenizer = self.transformers_tokenizer\n",
    "            \n",
    "            self.transformers_auto_tokenizer = AutoTokenizer.from_pretrained(\"inkoziev/sbert_pq\")\n",
    "            self.transformers_model = AutoModel.from_pretrained(\"inkoziev/sbert_pq\")\n",
    "            # self.model = SentenceTransformer(\"inkoziev/sbert_pq\")\n",
    "        else:\n",
    "            self.tokenizer = None\n",
    "            raise Exception(\"Invalid tokenizer.\")\n",
    "        \n",
    "        self.cosine_similarity = self.calc_similarity\n",
    "        # self.cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    def calc_similarity(self, vector1, vector2):\n",
    "        return np.dot(vector1, vector2) / \\\n",
    "               (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    def process(self, text: str):\n",
    "        aspects = []\n",
    "        # Схожесть\n",
    "        sentence_vector = self.tokenizer(text)\n",
    "        similarities = [(aspect, self.cosine_similarity(self.tokenizer(aspect), sentence_vector))\n",
    "                          for aspect in self.aspects_list]\n",
    "        # print(text)\n",
    "        # print(similarities)\n",
    "        for similarity in similarities:\n",
    "            if similarity[1] > self.min_similarity:\n",
    "                aspects.append(similarity[0])\n",
    "        return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d154d54d-ae45-4fe2-bb74-c0cf78030c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodNLI():\n",
    "\n",
    "    def set_aspects(self, aspects_list=None):\n",
    "        if aspects_list is not None:\n",
    "            self.aspects_list = aspects_list\n",
    "        else:\n",
    "            self.aspects_list = load_aspects()\n",
    "        self.prompts = self.aspects_list\n",
    "        # self.prompts = list(map(lambda w: \"Я сказал о \" + w, self.aspects_list))\n",
    "\n",
    "    def __init__(self, tokenizer=\"distiluse\", min_similarity = 0.5):\n",
    "        self.set_aspects(None)\n",
    "        self.min_similarity = min_similarity\n",
    "        model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "    def predict_zero_shot(self, text, label_texts, label='entailment', normalize=False):\n",
    "        label_texts\n",
    "        tokens = self.tokenizer([text] * len(label_texts), label_texts, truncation=True, return_tensors='pt', padding=True)\n",
    "        with torch.inference_mode():\n",
    "            result = torch.softmax(self.model(**tokens.to(self.model.device)).logits, -1)\n",
    "        proba = result[:, self.model.config.label2id[label]].cpu().numpy()\n",
    "        if normalize:\n",
    "            proba /= sum(proba)\n",
    "        return proba\n",
    "\n",
    "    def process(self, sentence: str):\n",
    "        aspects = []\n",
    "        similarities = self.predict_zero_shot(sentence, self.prompts)\n",
    "        for similarity, aspect in zip(similarities, self.aspects_list):\n",
    "            # print(f\"sim: {similarity} aspect: {aspect}\") DEBUG\n",
    "            if similarity > self.min_similarity:\n",
    "                aspects.append(aspect)\n",
    "        return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "072dfd00-e199-44bc-b958-7ece7b26c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_substring = MethodSubstring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f447c347-9a69-4fc7-b68f-9b3d0437754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_similarity = MethodSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4be29db1-d018-4d86-8707-fd9ee7baabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_nli = MethodNLI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b336550b-7c1c-4100-a01e-fdfafadf26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORTS_DIRECTORY = Path(\"../output/test-search-accuracy-reports\")\n",
    "REPORTS_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_report(method):\n",
    "    y_expected = []\n",
    "    y_predicted = []\n",
    "\n",
    "    for sentence in tqdm(all_aspects_senteces):\n",
    "        y_expected.append(\n",
    "            [\n",
    "                aspect\n",
    "                for aspect in actual_aspects_sentences\n",
    "                if sentence in actual_aspects_sentences[aspect]\n",
    "            ]\n",
    "        )\n",
    "        y_predicted.append(method.process(sentence))\n",
    "        # print(y_predicted) # DEBUG\n",
    "\n",
    "    y_expected = MultiLabelBinarizer(classes=list(actual_aspects_sentences)).fit_transform(y_expected)\n",
    "    y_predicted = MultiLabelBinarizer(classes=list(actual_aspects_sentences)).fit_transform(y_predicted)\n",
    "        \n",
    "    report = classification_report(y_expected, y_predicted, target_names=list(actual_aspects_sentences), output_dict=True)\n",
    "    return report\n",
    "\n",
    "def save_report(report, name):\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "# set precision for all numeric values to 2\n",
    "    df[['precision', 'recall', 'f1-score']] = df[['precision', 'recall', 'f1-score']].applymap(lambda x: round(x, 2) if isinstance(x, float) else x)\n",
    "\n",
    "    df.to_csv(REPORTS_DIRECTORY / (\"report_\" + name + \".csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f92552d1-c9a7-487d-86fa-98fccc3d419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating substring method accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:06<00:00, 120.12it/s]\n",
      "/home/danil/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/danil/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_6446/1810607091.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['precision', 'recall', 'f1-score']] = df[['precision', 'recall', 'f1-score']].applymap(lambda x: round(x, 2) if isinstance(x, float) else x)\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating substring method accuracy...\")\n",
    "report_substring = make_report(search_substring)\n",
    "save_report(report_substring, \"substring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b3e8eb22-fb24-4cc2-909c-9b9f71e24a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity meNthod accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [01:48<00:00,  7.41it/s]\n",
      "/home/danil/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_6446/1810607091.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['precision', 'recall', 'f1-score']] = df[['precision', 'recall', 'f1-score']].applymap(lambda x: round(x, 2) if isinstance(x, float) else x)\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating similarity meNthod accuracy...\")\n",
    "search_similarity.min_similarity = 0.30\n",
    "report_similarity = make_report(search_similarity)\n",
    "save_report(report_similarity, \"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9cf2ecdf-84dc-4248-a5a2-37d8b3679670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating NLI method accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:28<00:00, 28.32it/s]\n",
      "/home/danil/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_6446/1810607091.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['precision', 'recall', 'f1-score']] = df[['precision', 'recall', 'f1-score']].applymap(lambda x: round(x, 2) if isinstance(x, float) else x)\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating NLI method accuracy...\")\n",
    "search_nli.min_similarity = 0.65\n",
    "report_nli = make_report(search_nli)\n",
    "save_report(report_nli, \"nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867fd70-8191-4854-9651-b0aeb5a0e9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
