{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d80d1d06-5d69-49cb-9ae5-b0902438242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efcc22-e975-4c5a-a899-16c67daeb52b",
   "metadata": {},
   "source": [
    "# Подгружаем все размеченные аспекты из файлов и сохраняем в памяти\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "589d5229-dc7a-4ad6-a80a-36e40cf1cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['лекции', 'задания, задачи', 'практики, семинары', 'тесты', 'домашняя работа', 'материал, информация, темы', 'преподаватель'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIRECTORY = Path(\"../data/otzyvus-annotated/\")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "# Предварительно удаляем расщирение .txt\n",
    "aspect_file_names = [\n",
    "    f[:-4]\n",
    "        for f\n",
    "        in listdir(DATA_DIRECTORY)\n",
    "        if isfile(DATA_DIRECTORY / f) and \".txt\" in f\n",
    "]\n",
    "\n",
    "aspect_file_names = filter(\n",
    "    lambda name: name in [\n",
    "        \"преподаватель\",\n",
    "        \"материал__информация__темы\",\n",
    "        \"практики__семинары\",\n",
    "        \"лекции\",\n",
    "        \"тесты\",\n",
    "        \"задания__задачи\",\n",
    "        \"домашняя работа\"\n",
    "    ],\n",
    "    aspect_file_names\n",
    ")\n",
    "\n",
    "# Объект аспект - массив предложений\n",
    "actual_aspects_sentences = {}\n",
    "# Список всех уникальных предложений\n",
    "all_aspects_senteces = []\n",
    "for file_name in aspect_file_names:\n",
    "    with open(DATA_DIRECTORY / (file_name + \".txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        key = re.sub(r'[_]+', r', ', file_name).lower()\n",
    "        actual_aspects_sentences[key] = set(\n",
    "            filter(\n",
    "                lambda string: len(string) > 0 and string[0].isalpha(),\n",
    "                f.read().split('\\n')\n",
    "            )\n",
    "        )\n",
    "        all_aspects_senteces += actual_aspects_sentences[key]\n",
    "\n",
    "all_aspects_senteces = list(set(all_aspects_senteces))\n",
    "# actual_aspects_sentences.pop(\"мусор\")\n",
    "# print(actual_aspects_sentences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c95202cb-0b55-4ac0-8a98-9bc51d03bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aspects():\n",
    "    return list(actual_aspects_sentences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ce967d6e-512d-4e4b-a9d4-9e35142fdab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    # Убираем ссылки\n",
    "    clean = re.sub(u'(http|ftp|https):\\/\\/[^ ]+', '', text)\n",
    "    # Убираем все неалфавитные символы кроме дефиса и апострофа\n",
    "    clean = re.sub(u'[^а-я^А-Я^\\w^\\-^\\']', ' ', clean)\n",
    "    # Убираем тире\n",
    "    clean = re.sub(u' - ', ' ', clean)\n",
    "    # Убираем дубликаты пробелов\n",
    "    clean = re.sub(u'\\s+', ' ', clean)\n",
    "    # Убираем пробелы в начале и в конце строки\n",
    "    clean = clean.strip().lower()\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a4d5b5da-4294-437f-ac05-e783e10d7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodSubstring():\n",
    "    dictionary = {\n",
    "        \"материал, информация, темы\": [\"материал\"],\n",
    "        \"домашняя работа\": [\"домашняя работа\"],\n",
    "        \"зачет, экзамен\": [\"зачет\"],\n",
    "        \"фильмы\": [\"фильм\", \"кино\"],\n",
    "        \"презентации\": [\"презентация\"],\n",
    "        \"онлайн-курс\": [\"онлайн-курс\"],\n",
    "        \"видео-уроки\": [\"видео-урок\", \"видеоурок\"],\n",
    "        \"преподаватель\": [\"преподаватель\", \"препод\"],\n",
    "        \"выступления\": [\"выступление\"],\n",
    "        \"литература\": [\"литература\", \"книга\"],\n",
    "        \"тесты\": [\"тест\"],\n",
    "        \"практики, семинары\": [\"практика\"],\n",
    "        \"доклады\": [\"доклад\"],\n",
    "        \"задания, задачи\": [\"задание\"],\n",
    "        \"баллы, оценки\": [\"балл\", \"оценка\"],\n",
    "        \"эссе\": [\"эссе\"],\n",
    "        \"проекты\": [\"проект\"],\n",
    "        \"игры, интерактивность\" : [\"игра\"],\n",
    "        \"лекции\": [\"лекция\"],\n",
    "    }\n",
    "\n",
    "    def set_aspects(self, aspects_list=None):\n",
    "        if aspects_list is not None:\n",
    "            self.aspects_list = aspects_list\n",
    "        else:\n",
    "            self.aspects_list = load_aspects()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.set_aspects(None)\n",
    "\n",
    "    def find_aspects(self, sentence: str):\n",
    "        aspects = []\n",
    "        # Очистка\n",
    "        sentence_words = clean_text(sentence).split(\" \")\n",
    "        # Лемматизация\n",
    "        doc = Doc(sentence)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "\n",
    "        for token in doc.tokens: \n",
    "            token.lemmatize(morph_vocab)\n",
    "        \n",
    "        lemmatized = ' '.join(token.lemma for token in doc.tokens)\n",
    "        \n",
    "        for aspect in self.aspects_list:\n",
    "            for word in self.dictionary[aspect]:\n",
    "                if word in lemmatized:\n",
    "                    aspects.append(aspect)\n",
    "        return aspects\n",
    "\n",
    "    def process(self, text: str):\n",
    "        return self.find_aspects(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "445b263d-cd19-4067-96a0-8d736eff99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodSimilarity():\n",
    "\n",
    "    def transformers_tokenizer(self, sentence: str) -> torch.Tensor:\n",
    "        \n",
    "        tokens = self.transformers_auto_tokenizer(sentence, return_tensors='pt')\n",
    "        vector = self.transformers_model(**tokens)[0].detach().squeeze()\n",
    "        return torch.mean(vector, dim=0).numpy()\n",
    "        # return self.model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    def set_aspects(self, aspects_list=None):\n",
    "        if aspects_list is not None:\n",
    "            self.aspects_list = aspects_list\n",
    "        else:\n",
    "            self.aspects_list = load_aspects()\n",
    "\n",
    "    def __init__(self, tokenizer=\"distiluse\", min_similarity = 0.3):\n",
    "        self.set_aspects(None)\n",
    "        self.aspects_list = load_aspects()\n",
    "        self.min_similarity = min_similarity\n",
    "        if tokenizer == \"distiluse\":\n",
    "            self.tokenizer = self.transformers_tokenizer\n",
    "            \n",
    "            self.transformers_auto_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "            self.transformers_model = AutoModel.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "            # self.model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "        elif tokenizer == \"sbert-pq\":\n",
    "            self.tokenizer = self.transformers_tokenizer\n",
    "            \n",
    "            self.transformers_auto_tokenizer = AutoTokenizer.from_pretrained(\"inkoziev/sbert_pq\")\n",
    "            self.transformers_model = AutoModel.from_pretrained(\"inkoziev/sbert_pq\")\n",
    "            # self.model = SentenceTransformer(\"inkoziev/sbert_pq\")\n",
    "        else:\n",
    "            self.tokenizer = None\n",
    "            raise Exception(\"Invalid tokenizer.\")\n",
    "        \n",
    "        self.cosine_similarity = self.calc_similarity\n",
    "        # self.cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    def calc_similarity(self, vector1, vector2):\n",
    "        return np.dot(vector1, vector2) / \\\n",
    "               (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    def process(self, text: str):\n",
    "        aspects = []\n",
    "        # Схожесть\n",
    "        sentence_vector = self.tokenizer(text)\n",
    "        similarities = [(aspect, self.cosine_similarity(self.tokenizer(aspect), sentence_vector))\n",
    "                          for aspect in self.aspects_list]\n",
    "        # print(text)\n",
    "        # print(similarities)\n",
    "        for similarity in similarities:\n",
    "            if similarity[1] > self.min_similarity:\n",
    "                aspects.append(similarity[0])\n",
    "        return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d154d54d-ae45-4fe2-bb74-c0cf78030c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodNLI():\n",
    "\n",
    "    def set_aspects(self, aspects_list=None):\n",
    "        if aspects_list is not None:\n",
    "            self.aspects_list = aspects_list\n",
    "        else:\n",
    "            self.aspects_list = load_aspects()\n",
    "        self.prompts = self.aspects_list\n",
    "        # self.prompts = list(map(lambda w: \"Я сказал о \" + w, self.aspects_list))\n",
    "\n",
    "    def __init__(self, tokenizer=\"distiluse\", min_similarity = 0.5):\n",
    "        self.set_aspects(None)\n",
    "        self.min_similarity = min_similarity\n",
    "        model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "    def predict_zero_shot(self, text, label_texts, label='entailment', normalize=False):\n",
    "        label_texts\n",
    "        tokens = self.tokenizer([text] * len(label_texts), label_texts, truncation=True, return_tensors='pt', padding=True)\n",
    "        with torch.inference_mode():\n",
    "            result = torch.softmax(self.model(**tokens.to(self.model.device)).logits, -1)\n",
    "        proba = result[:, self.model.config.label2id[label]].cpu().numpy()\n",
    "        if normalize:\n",
    "            proba /= sum(proba)\n",
    "        return proba\n",
    "\n",
    "    def process(self, sentence: str):\n",
    "        aspects = []\n",
    "        similarities = self.predict_zero_shot(sentence, self.prompts)\n",
    "        for similarity, aspect in zip(similarities, self.aspects_list):\n",
    "            # print(f\"sim: {similarity} aspect: {aspect}\") DEBUG\n",
    "            if similarity > self.min_similarity:\n",
    "                aspects.append(aspect)\n",
    "        return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "072dfd00-e199-44bc-b958-7ece7b26c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_substring = MethodSubstring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f447c347-9a69-4fc7-b68f-9b3d0437754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_similarity = MethodSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4be29db1-d018-4d86-8707-fd9ee7baabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_nli = MethodNLI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b336550b-7c1c-4100-a01e-fdfafadf26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORTS_DIRECTORY = Path(\"../output/test-search-accuracy-reports\")\n",
    "REPORTS_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_report(method):\n",
    "    y_expected = []\n",
    "    y_predicted = []\n",
    "\n",
    "    for sentence in tqdm(all_aspects_senteces):\n",
    "        y_expected.append(\n",
    "            [\n",
    "                aspect\n",
    "                for aspect in actual_aspects_sentences\n",
    "                if sentence in actual_aspects_sentences[aspect]\n",
    "            ]\n",
    "        )\n",
    "        y_predicted.append(method.process(sentence))\n",
    "        # print(y_predicted) # DEBUG\n",
    "\n",
    "    y_expected = MultiLabelBinarizer(classes=list(actual_aspects_sentences)).fit_transform(y_expected)\n",
    "    y_predicted = MultiLabelBinarizer(classes=list(actual_aspects_sentences)).fit_transform(y_predicted)\n",
    "        \n",
    "    report = classification_report(y_expected, y_predicted, target_names=list(actual_aspects_sentences), output_dict=True)\n",
    "    return report\n",
    "\n",
    "def save_report(report, name):\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "# set precision for all numeric values to 2\n",
    "    df[['precision', 'recall', 'f1-score']] = df[['precision', 'recall', 'f1-score']].applymap(lambda x: round(x, 2) if isinstance(x, float) else x)\n",
    "\n",
    "    df.to_csv(REPORTS_DIRECTORY / (\"report_\" + name + \".csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f92552d1-c9a7-487d-86fa-98fccc3d419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating substring method accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/806 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'задания, задачи'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating substring method accuracy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m report_substring \u001b[38;5;241m=\u001b[39m \u001b[43mmake_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_substring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m save_report(report_substring, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[122], line 16\u001b[0m, in \u001b[0;36mmake_report\u001b[0;34m(method)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(all_aspects_senteces):\n\u001b[1;32m      9\u001b[0m     y_expected\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     10\u001b[0m         [\n\u001b[1;32m     11\u001b[0m             aspect\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         ]\n\u001b[1;32m     15\u001b[0m     )\n\u001b[0;32m---> 16\u001b[0m     y_predicted\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(y_predicted) # DEBUG\u001b[39;00m\n\u001b[1;32m     19\u001b[0m y_expected \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer(classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(actual_aspects_sentences))\u001b[38;5;241m.\u001b[39mfit_transform(y_expected)\n",
      "Cell \u001b[0;32mIn[115], line 54\u001b[0m, in \u001b[0;36mMethodSubstring.process\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_aspects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[115], line 48\u001b[0m, in \u001b[0;36mMethodSubstring.find_aspects\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     45\u001b[0m lemmatized \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(token\u001b[38;5;241m.\u001b[39mlemma \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m aspect \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maspects_list:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m lemmatized:\n\u001b[1;32m     50\u001b[0m             aspects\u001b[38;5;241m.\u001b[39mappend(aspect)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'задания, задачи'"
     ]
    }
   ],
   "source": [
    "print(\"Calculating substring method accuracy...\")\n",
    "report_substring = make_report(search_substring)\n",
    "save_report(report_substring, \"substring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b3e8eb22-fb24-4cc2-909c-9b9f71e24a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity meNthod accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                                                                                     | 7/806 [00:01<02:03,  6.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating similarity meNthod accuracy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m search_similarity\u001b[38;5;241m.\u001b[39mmin_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.30\u001b[39m\n\u001b[0;32m----> 3\u001b[0m report_similarity \u001b[38;5;241m=\u001b[39m \u001b[43mmake_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_similarity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m save_report(report_similarity, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[122], line 16\u001b[0m, in \u001b[0;36mmake_report\u001b[0;34m(method)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(all_aspects_senteces):\n\u001b[1;32m      9\u001b[0m     y_expected\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     10\u001b[0m         [\n\u001b[1;32m     11\u001b[0m             aspect\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         ]\n\u001b[1;32m     15\u001b[0m     )\n\u001b[0;32m---> 16\u001b[0m     y_predicted\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(y_predicted) # DEBUG\u001b[39;00m\n\u001b[1;32m     19\u001b[0m y_expected \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer(classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(actual_aspects_sentences))\u001b[38;5;241m.\u001b[39mfit_transform(y_expected)\n",
      "Cell \u001b[0;32mIn[116], line 47\u001b[0m, in \u001b[0;36mMethodSimilarity.process\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Схожесть\u001b[39;00m\n\u001b[1;32m     46\u001b[0m sentence_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n\u001b[0;32m---> 47\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [(aspect, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosine_similarity(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(aspect), sentence_vector))\n\u001b[1;32m     48\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m aspect \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maspects_list]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# print(similarities)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m similarity \u001b[38;5;129;01min\u001b[39;00m similarities:\n",
      "Cell \u001b[0;32mIn[116], line 47\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Схожесть\u001b[39;00m\n\u001b[1;32m     46\u001b[0m sentence_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n\u001b[0;32m---> 47\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [(aspect, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosine_similarity(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m)\u001b[49m, sentence_vector))\n\u001b[1;32m     48\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m aspect \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maspects_list]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# print(similarities)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m similarity \u001b[38;5;129;01min\u001b[39;00m similarities:\n",
      "Cell \u001b[0;32mIn[116], line 6\u001b[0m, in \u001b[0;36mMethodSimilarity.transformers_tokenizer\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformers_tokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers_auto_tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(vector, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:822\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m--> 822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:587\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    580\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    581\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m         output_attentions,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:513\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    522\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/abas-study-feedbacks-research-KcRJMMgI-py3.10/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:256\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 256\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m context \u001b[38;5;241m=\u001b[39m unshape(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Calculating similarity meNthod accuracy...\")\n",
    "search_similarity.min_similarity = 0.30\n",
    "report_similarity = make_report(search_similarity)\n",
    "save_report(report_similarity, \"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2ecdf-84dc-4248-a5a2-37d8b3679670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating NLI method accuracy...\")\n",
    "search_nli.min_similarity = 0.65\n",
    "report_nli = make_report(search_nli)\n",
    "save_report(report_nli, \"nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867fd70-8191-4854-9651-b0aeb5a0e9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
